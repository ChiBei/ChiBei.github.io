---
title: 机器学习笔记
tags: [机器学习]
style: fill
color: light
description: 机器学习笔记，知识框架。
---

## 监督学习

• 训练集中每个样本都有一个类别标记
• 所有类别事先已知

### 回归

用于研究自变量与因变量之间的关系

### 分类

#### 朴素贝叶斯

先验概率：the initial guess
朴素 naive：忽略了语言的顺序等（难以全），只看出现的概率

![image-20211023134020202](https://raw.githubusercontent.com/ChiBei/Pictures/master/notesimage-20211023134020202.png)

属性拆分为多个，比如大小、颜色。假定特征向量的各分量间相对于决策变量是相对独立的，也就是说各分量独立地作用于决策变量。

<img src="https://raw.githubusercontent.com/ChiBei/Pictures/master/notesimage-20211023134040452.png" alt="image-20211023134040452" style="zoom:50%;" />				

![image-20211023140836026](https://raw.githubusercontent.com/ChiBei/Pictures/master/notesimage-20211023140836026.png)

![image-20211023134450064](https://raw.githubusercontent.com/ChiBei/Pictures/master/notesimage-20211023134450064.png)

#### K-NN K-近邻算法

相似度度量

<img src="https://raw.githubusercontent.com/ChiBei/Pictures/master/notesimage-20211023134538768.png" alt="image-20211023134538768" style="zoom: 50%;" />				

![image-20211023134555450](https://raw.githubusercontent.com/ChiBei/Pictures/master/notesimage-20211023134555450.png)

#### SVM 支持向量机

最优分类面：要求分类面能将两类正确分开(训练错误率为0),且使分类间隔最大

![image-20211023134617845](https://raw.githubusercontent.com/ChiBei/Pictures/master/notesimage-20211023134617845.png)

支持向量：

•支持向量是两类集合边界上的点。
•所有非支持向量的数据都可以从训练数据集合中去掉而不影响问题解的结果。

![image-20211023140752436](https://raw.githubusercontent.com/ChiBei/Pictures/master/notesimage-20211023140752436.png)

#### 决策树

非叶子节点上是 属性

![image-20211023140739107](https://raw.githubusercontent.com/ChiBei/Pictures/master/notesimage-20211023140739107.png)

------

## 无监督学习

• 训练集中样本的类别标记未知
• 给定一组样本，发现其内在性质，如类别和聚类

### 聚类

#### K-means

![image-20211023142156458](https://raw.githubusercontent.com/ChiBei/Pictures/master/notesimage-20211023142156458.png)

![image-20211023142206161](https://raw.githubusercontent.com/ChiBei/Pictures/master/notesimage-20211023142206161.png)

![image-20211023142221360](https://raw.githubusercontent.com/ChiBei/Pictures/master/notesimage-20211023142221360.png)			

![image-20211023140704196](https://raw.githubusercontent.com/ChiBei/Pictures/master/notesimage-20211023140704196.png)

![image-20211023140717483](https://raw.githubusercontent.com/ChiBei/Pictures/master/notesimage-20211023140717483.png)

### 降维

在降低数据维度的同时，保证其中包含的主要信息是相似的（有效信息不丢失），主要包括特征抽取的方法和特征选择的方法。

#### PCA 主成分分析

数据降维后数据尽可能分散
归结为找角度：

![image-20211023140408034](https://raw.githubusercontent.com/ChiBei/Pictures/master/notesimage-20211023140408034.png)					

![image-20211023134902512](https://raw.githubusercontent.com/ChiBei/Pictures/master/notesimage-20211023134902512.png)

![image-20211023134907396](https://raw.githubusercontent.com/ChiBei/Pictures/master/notesimage-20211023134907396.png)

![image-20211023134911348](https://raw.githubusercontent.com/ChiBei/Pictures/master/notesimage-20211023134911348.png)

